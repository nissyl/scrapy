{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib, requests, csv, re, time, socks,socket, sys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from requests import ConnectionError,Timeout\n",
    "from urllib.error import HTTPError, URLError\n",
    "from ssl import SSLError\n",
    "import http.client\n",
    "import chardet\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from socks import SOCKS5Error #!pip install PySocks\n",
    "socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, \"127.0.0.1\", 9150)\n",
    "socket.socket = socks.socksocket\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('01.csv', 'r')#CSVファイルの読み込み\n",
    "dataReader = csv.reader(f)#CSVファイルを読める形に置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-6cc9e85bb49c>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6cc9e85bb49c>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    except requests.exceptions.Timeout:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#結果の出力用のデータフレームを作る。\n",
    "data1 =[] #詳細のURL\n",
    "\n",
    "for row in dataReader:   #URLの入ったCSVを1行ずつ読み込んでくれる\n",
    "       for url in row:   #CSVの1行に入っているURLを1行ずつ読み込む\n",
    "            company_name_sub=BeautifulSoup('None', \"html.parser\")\n",
    "\n",
    "            for row in dataReader:   #URLの入ったCSVを1行ずつ読み込んでくれる\n",
    "                   for url in row:   #CSVの1行に入っているURLを1行ずつ読み込む\n",
    "                        company_name_sub=BeautifulSoup('None', \"html.parser\")\n",
    "\n",
    "                        r = requests.get(url,timeout=60)#urlから、htmlの情報を取得する。 \n",
    "                        soup = BeautifulSoup(r.content, 'html.parser')#htmlファイルのデータをPythonで整形・抽出しやすくするためにbsオブジェクトにする。\n",
    "\n",
    "                        for estate_info in soup.findAll(\"div\",{\"class\":\"u_title\"}):\n",
    "                            for link in estate_info.findAll('a'):\n",
    "                                if 'domestic' in link.get(\"href\"):\n",
    "                                    data1.append(link.get(\"href\")) \n",
    "                                    break\n",
    "                    \n",
    "                                                    \n",
    "           # except HTTPError as e:\n",
    "             #   print(e.code)\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(\"Timeout occurred\")\n",
    "                \n",
    "            except ConnectionError:\n",
    "                print(\"ConnectionError\")\n",
    "                    \n",
    "            except SSLError:\n",
    "                print(\"SSLError\")\n",
    "                    \n",
    "            except TypeError:\n",
    "                print(\"TypeError\")\n",
    "                    \n",
    "            except URLError:\n",
    "                print(\"URLError\")\n",
    "                    \n",
    "            except  ValueError:\n",
    "                print(\"ValueError\")\n",
    "                    \n",
    "            except socket.timeout:\n",
    "                print(\"Timeout occurred\")\n",
    "                    \n",
    "            except KeyError as instance:\n",
    "                print(instance)\n",
    "                    \n",
    "            except ConnectionResetError:\n",
    "                print(\"ConnectionResetError\")\n",
    "                    \n",
    "            except http.client.IncompleteRead:\n",
    "                print(\"IncompleteRead\")\n",
    "                                        \n",
    "            except http.client.HTTPException:\n",
    "                print(\"HTTPException\")\n",
    "                \n",
    "            except NotImplementedError:\n",
    "                print(\"NotImplementedError\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = zip(data01)\n",
    "#CSV出力\n",
    "with open('trip_result_link.csv','wt',errors='backslashreplace') as fout:\n",
    "    writecsv = csv.writer(fout,lineterminator='\\n')\n",
    "    writecsv.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('trip_result.csv', 'r')#CSVファイルの読み込み\n",
    "dataReader = csv.reader(f)#CSVファイルを読める形に置き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#結果の出力用のデータフレームを作る。\n",
    "data01 =[] #詳細のURL\n",
    "data02 =[]\n",
    "\n",
    "for row in dataReader:   #URLの入ったCSVを1行ずつ読み込んでくれる\n",
    "       for url in row:   #CSVの1行に入っているURLを1行ずつ読み込む\n",
    "            company_name_sub=BeautifulSoup('None', \"html.parser\")\n",
    "\n",
    "            for row in dataReader:   #URLの入ったCSVを1行ずつ読み込んでくれる\n",
    "                   for url in row:   #CSVの1行に入っているURLを1行ずつ読み込む\n",
    "                        company_name_sub=BeautifulSoup('None', \"html.parser\")\n",
    "\n",
    "                        r = requests.get(url,timeout=60)#urlから、htmlの情報を取得する。 \n",
    "                        soup = BeautifulSoup(r.content, 'html.parser')#htmlファイルのデータをPythonで整形・抽出しやすくするためにbsオブジェクトにする。\n",
    "                        \n",
    "                        a = soup.find(\"h1\")\n",
    "                        data01.append(a.getText())\n",
    "\n",
    "                        for estate_info in soup.findAll(\"div\",{\"id\":\"main\"}):\n",
    "                            for link in estate_info.findAll('p'):\n",
    "                                data02.append(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = zip(data01,data02)\n",
    "#CSV出力\n",
    "with open('trip_result.csv','wt',errors='backslashreplace') as fout:\n",
    "    writecsv = csv.writer(fout,lineterminator='\\n')\n",
    "    writecsv.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
