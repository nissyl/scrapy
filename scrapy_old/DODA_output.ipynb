{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, requests,regex\n",
    "import csv #CSVを扱えるようにするため\n",
    "import re #文字列処理をしやすくするため\n",
    "import time #処理を一時的に止めるため\n",
    "import socks,socket #IPを偽装するため\n",
    "import sys #ファイルのパスの情報を扱うため\n",
    "from bs4 import BeautifulSoup #タグなどの抽出のため\n",
    "from urllib.request import urlopen #情報取得のため\n",
    "from requests import ConnectionError,Timeout #エラーが起きた際に回避するため\n",
    "from urllib.error import HTTPError, URLError #エラーが起きた際に回避するため\n",
    "from ssl import SSLError #エラーが起きた際に回避するため\n",
    "import http.client #情報取得のため\n",
    "import chardet #文字列を扱いやすくするライブラリ\n",
    "from socks import SOCKS5Error #!pip install PySocks #エラーが起きた際に回避するため\n",
    "socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, \"127.0.0.1\", 9150) #IPを偽装するため\n",
    "socket.socket = socks.socksocket #IPを偽装するため\n",
    "sys.setrecursionlimit(10000) #処理量の多くなるようなサイトの時に取得が途中で止まらないようにするため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_database = pd.DataFrame(columns=[\"url\",\"company_name\",\"company_url\",\"job\",\"address\",\"establish\",\"capital\",\"contact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list_1 = pd.read_csv(\"url_company_se_list.csv\",header=None)\n",
    "url_list_2 = pd.read_csv(\"url_company_soft_list.csv\",header=None)\n",
    "url_list_3 = pd.read_csv(\"url_company_creative_list.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　リスト3つを繋がる\n",
    "url_list_all = url_list_1.append(url_list_2, ignore_index=True)\n",
    "url_list_all = url_list_all.append(url_list_3, ignore_index=True)\n",
    "url_list_all.columns = [\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すでに取得したものを除外する\n",
    "extracted_result = pd.read_pickle(\"result_database.pickle\")\n",
    "url_list_all = url_list_all[~url_list_all.url.isin(extracted_result.url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for url in url_list_all[0]:\n",
    "\n",
    "\n",
    "    company_url_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    company_name_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    contact_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    job_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    address_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    establish_sub=BeautifulSoup('None', \"html.parser\")\n",
    "    capital_sub=BeautifulSoup('None', \"html.parser\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url,timeout=60,allow_redirects=False)                   \n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        remove_expression = \"\\r|\\n|\\t\"\n",
    "\n",
    "        for company_name in soup.findAll('h1',{'class':'iconDisp'}):\n",
    "            company_name_sub = company_name.text            \n",
    "            company_name_sub = re.sub(remove_expression,\"\",company_name_sub)\n",
    "\n",
    "        for table in soup.findAll('div',{'class':'modDetail02'}):\n",
    "            for company_url in table.findAll('dl'):\n",
    "                for parts in company_url.findAll('dt'):\n",
    "                    if regex.findall(\"企業URL\",parts.text):\n",
    "                        company_url_sub = company_url.find('dd')\n",
    "                        company_url_sub = company_url_sub.find('a')\n",
    "                        company_url_sub = company_url_sub.get('href')    \n",
    "\n",
    "\n",
    "        for job in soup.findAll('p',{'class':'fs15 bold'}):\n",
    "            job_sub = job.text\n",
    "            job_sub =re.sub(remove_expression,\"\",job_sub) #イケてない文字列を削除する\n",
    "\n",
    "\n",
    "        for table in soup.findAll('div',{'class':'modDetail04'}):\n",
    "            for address in table.findAll('dl'):\n",
    "                for parts in address.find('dt'):\n",
    "                    if regex.findall(\"所在地\",parts):\n",
    "                        address_sub = address.find('dd')\n",
    "                        address_sub = address_sub.text\n",
    "                        address_sub =re.sub(remove_expression,\"\",address_sub) #イケてない文字列を削除する\n",
    "\n",
    "\n",
    "        for table in soup.findAll('div',{'class':'modDetail04'}):\n",
    "            for establish in table.findAll('dl'):\n",
    "                for parts in establish.find('dt'):\n",
    "                    if regex.findall(\"設立\",parts):\n",
    "                        establish_sub = establish.find('dd')\n",
    "                        establish_sub = establish_sub.text\n",
    "                        establish_sub =re.sub(remove_expression,\"\",establish_sub) #イケてない文字列を削除する\n",
    "\n",
    "        for table in soup.findAll('div',{'class':'modDetail04'}):\n",
    "            for capital in table.findAll('dl'):\n",
    "                for parts in capital.find('dt'):\n",
    "                    if regex.findall(\"資本金\",parts):\n",
    "                        capital = capital.find('dd')\n",
    "                        capital_sub =capital.text\n",
    "                        capital_sub =re.sub(remove_expression,\"\",capital_sub) #イケてない文字列を削除する\n",
    "\n",
    "        for table in soup.findAll('div',{'class':'modDetail02'}):\n",
    "            for contact in table.findAll('dl'):\n",
    "                if regex.findall(\"連絡先\",contact.text):\n",
    "                    contact_sub = contact.text\n",
    "                    contact_sub =re.sub(remove_expression,\"\",contact_sub) #イケてない文字列を削除する\n",
    "\n",
    "        df = pd.DataFrame([[url,company_name_sub,company_url_sub,job_sub,address_sub,establish_sub,capital_sub,contact_sub]],columns=result_database.columns)            \n",
    "        result_database = result_database.append(df).reset_index(drop=True) #インデックスをつける。付けないと何番目か分からない。\n",
    "        result_database.to_pickle(\"result_database.pickle\")\n",
    "    \n",
    "    \n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(e.code)\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timeout occurred\")\n",
    "\n",
    "    except ConnectionError:\n",
    "        print(\"ConnectionError\")\n",
    "\n",
    "    except SSLError:\n",
    "        print(\"SSLError\")\n",
    "\n",
    "    except TypeError:\n",
    "        print(\"TypeError\")\n",
    "\n",
    "    except URLError:\n",
    "        print(\"URLError\")\n",
    "\n",
    "    except  ValueError:\n",
    "        print(\"ValueError\")\n",
    "\n",
    "    except socket.timeout:\n",
    "        print(\"Timeout occurred\")\n",
    "\n",
    "    except KeyError as instance:\n",
    "        print(instance)\n",
    "\n",
    "    except ConnectionResetError:\n",
    "        print(\"ConnectionResetError\")\n",
    "\n",
    "    except http.client.IncompleteRead:\n",
    "        print(\"IncompleteRead\")\n",
    "\n",
    "    except http.client.HTTPException:\n",
    "        print(\"HTTPException\")\n",
    "\n",
    "    except NotImplementedError:\n",
    "        print(\"NotImplementedError\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
