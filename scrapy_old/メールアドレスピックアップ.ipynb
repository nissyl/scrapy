{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString, Declaration, Comment\n",
    "from urllib.request import urlopen\n",
    "from requests import ConnectionError\n",
    "from requests.exceptions import ContentDecodingError\n",
    "from urllib.error import HTTPError, URLError\n",
    "import csv, re, time, http.client, chardet, sys\n",
    "from ssl import SSLError\n",
    "\n",
    "#不要なタグの定義など\n",
    "def getNavigableStrings(soup):\n",
    "    if isinstance(soup, NavigableString):\n",
    "        if type(soup) not in (Comment, Declaration) and soup.strip():\n",
    "            yield soup\n",
    "    elif soup.name not in ('script', 'style'):\n",
    "        for c in soup.contents:\n",
    "            for g in getNavigableStrings(c):\n",
    "                yield g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_condition = '.png|.jpg|.jpeg|.pdf'\n",
    "f = open('link_relational_data.csv', 'r')\n",
    "dataReader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#結果の出力用のデータフレームを作る。\n",
    "data01 =[]\n",
    "data02 =[]\n",
    "data03 =[]\n",
    "data04 =[]\n",
    "data05 =[]\n",
    "data06 =[]\n",
    "data07 =[]\n",
    "data08 =[]\n",
    "\n",
    "p = re.compile(r\"<[^>]*?>\")\n",
    "\n",
    "for parts in dataReader:\n",
    "    for site_url in parts:\n",
    "            time.sleep(1.0) #sleep(秒指定)\n",
    "            print(site_url)\n",
    "            try:\n",
    "                        r = requests.get(site_url,allow_redirects=True)\n",
    "                        if r.status_code == 200:\n",
    "                            \n",
    "                            soup = BeautifulSoup(r.content)\n",
    "                            #soup = BeautifulSoup(r.text)\n",
    "                            soup = BeautifulSoup(str(soup).replace(\"\\r\", \" \"))\n",
    "                            text = ' '.join(getNavigableStrings(soup))\n",
    "                            text = p.sub(\"\", text)\n",
    "                            text = text.replace(\",\",\"\")\n",
    "                            text = text.replace(\"\\t\",\"\")\n",
    "                            text = text[0:10000]\n",
    "                                            \n",
    "                            for link in soup.findAll(\"a\"):\n",
    "                                try:\n",
    "                                    if re.match(site_url,link.get(\"href\")):\n",
    "                                        url = link.get(\"href\")\n",
    "                                    if not re.match(\"http\",link.get(\"href\")):\n",
    "                                        if re.match(\"^/\",link.get(\"href\")):\n",
    "                                            url = site_url + link.get(\"href\")\n",
    "                                        if not re.match(\"^/\",link.get(\"href\")):\n",
    "                                            url = site_url + \"/\" + link.get(\"href\")\n",
    "                                      \n",
    "                                    if not re.search(remove_condition,url):\n",
    "                                        #URL先を取得する\n",
    "                                        time.sleep(3.0) #sleep(秒指定)\n",
    "                                        url = url.replace(\"/./\",\"/\")\n",
    "                                        r2 = requests.get(url,allow_redirects=True)\n",
    "                                        soup2 = BeautifulSoup(r2.content, 'html.parser')\n",
    "                                        soup2 = BeautifulSoup(str(soup2).replace(\"\\r\", \" \"))\n",
    "                                        text2 = ' '.join(getNavigableStrings(soup2))\n",
    "                                        text2 = p.sub(\"\", text2)\n",
    "                                        text2 = text2.replace(\",\",\"\")                                        \n",
    "                                        text2 = text2.replace(\"\\t\",\"\") \n",
    "                                        text2 = text2[0:10000]\n",
    "                                    \n",
    "                                        #データの格納\n",
    "                                        data01.append(site_url)\n",
    "                                        data02.append(text)\n",
    "                                        data03.append(''.join(link.findAll(text=True)))                                    \n",
    "                                        data04.append(url)\n",
    "                                        data05.append(text2)\n",
    "                                        data06.append(r.status_code)\n",
    "                                        data07.append(r2.status_code)                                    \n",
    "                                        data08.append(\"linklist\")\n",
    "                                        data = zip(data01,data02,data03,data04,data05,data06,data07,data08)\n",
    "                                        #CSV出力\n",
    "                                        with open('Company_site_Text_result.csv','wt',errors='backslashreplace') as fout:\n",
    "                                            writecsv = csv.writer(fout,lineterminator='\\n')\n",
    "                                            writecsv.writerows(data)                                   \n",
    "                                \n",
    "                                except TypeError:\n",
    "                                    a = []\n",
    "                            \n",
    "            except HTTPError as e:\n",
    "                print(e.code)\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(\"Timeout occurred\")\n",
    "                \n",
    "            except ConnectionError:\n",
    "                print(\"ConnectionError\")\n",
    "                    \n",
    "            except SSLError:\n",
    "                print(\"SSLError\")\n",
    "                    \n",
    "            except TypeError:\n",
    "                print(\"TypeError\")\n",
    "                    \n",
    "            except URLError:\n",
    "                print(\"URLError\")\n",
    "                    \n",
    "            except  ValueError:\n",
    "                print(\"ValueError\")\n",
    "                    \n",
    "            except socket.timeout:\n",
    "                print(\"Timeout occurred\")\n",
    "                    \n",
    "            except KeyError as instance:\n",
    "                print(instance)\n",
    "                    \n",
    "            except ConnectionResetError:\n",
    "                print(\"ConnectionResetError\")\n",
    "                    \n",
    "            except http.client.IncompleteRead:\n",
    "                print(\"IncompleteRead\")\n",
    "                                        \n",
    "            except http.client.HTTPException:\n",
    "                print(\"HTTPException\")\n",
    "                \n",
    "            except NotImplementedError:\n",
    "                print(\"NotImplementedError\")\n",
    "                \n",
    "            except requests.exceptions.TooManyRedirects:\n",
    "                print(\"TooManyRedirects\")\n",
    "                \n",
    "            except RecursionError:\n",
    "                print(\"RecursionError\")\n",
    "                \n",
    "            except ContentDecodingError:\n",
    "                print(\"ContentDecodingError\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn, sys, codecs\n",
    "sys.setrecursionlimit(10000)\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "#保存したデータの読み込み\n",
    "df = pd.read_csv(open('Company_site_Text_result.csv','r'), encoding='utf-8', engine='c',header=None) #CSVでデータを読み込む\n",
    "df.columns = ['url', 'text','anchor_text','link','target_text','status_code','target_status_code','partition'] #列名を付ける。\n",
    "df.tail(10) #10件表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#対象の絞込\n",
    "keyword1 = r'お問い合わせ|お問合せ'\n",
    "keyword2 = r''\n",
    "keyword3 = r'\\@|＠'\n",
    "\n",
    "df_filtered = df[df['target_text'].str.contains(keyword1,na=False)]\n",
    "df_filtered = df_filtered[df_filtered['target_text'].str.contains(keyword2,na=False)]\n",
    "df_filtered = df_filtered[df_filtered['target_text'].str.contains(keyword3,na=False)]\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Emailアドレスを正規表現で抽出\n",
    "df_filtered['email'] = df_filtered['target_text'].str.extract('([\\w\\.-]+@[\\w\\.-]+)')\n",
    "email_extract_company = df_filtered[df_filtered['email'].notnull()]\n",
    "email_extract_company = email_extract_company[['url','link','email']]\n",
    "email_extract_company = email_extract_company.drop_duplicates(['url','email'])\n",
    "email_extract_company.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#取得したホームページに占めるメールアドレスらしきホームページの割合（企業ユニーク）\n",
    "print(str(round(len(email_extract_company.url.value_counts())/len(df.url.value_counts())*100,2)) + \"％\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CSV出力\n",
    "email_extract_company.to_csv(\"email_extract_list.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
